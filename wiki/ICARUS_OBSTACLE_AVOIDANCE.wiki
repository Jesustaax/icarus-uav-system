#summary One-sentence summary of this page.

[ICARUS_OBSTACLE_AVOIDANCE#Operation Operating Instructions]
[https://bitbucket.org/uicrobotics/icarus_oa ROS Source Code]
[https://www.dropbox.com/sh/5k0mwuku06bbsmt/agLkb_57Lh Propeller Source Code]

[PrimaryController Primary Controller]
[FlightController Flight Controller]
[MotionController Motion Controller]


= Tasks =
 * CAD Conceptual Design (DONE)
 * Dev Primary and Motion Controllers to work with current Mode FSM. (DONE)
 * Fuse AI-OA_Sonic and AI-OA_Kinect to provide better Obstacle Avoidance maneuvers.

== OA_Kinect ==
 * Dev Code to acquire (and save on mouseclick) Disparity Images (DONE)
 * Develop appropriate Image Filter
 * Develop AI algorithm (AI-OA_Kinect) relating Kinect tracked obstacles and input/output motor commands. 

== OA_Sonic ==
 * Dev Code for MC to measure distances, receive motor commands and output motor commands. (DONE)
 * Develop AI algorithm (AI-OA_Sonic) relating Ultrasonic distances and input/output motor commands.

= Milestones =

== OA_Kinect ==
  * Acquire images with Kinect into a Laptop running Ubuntu/ROS (DONE)
  * Compare/contrast different imaging capabilities on the Kinect to find the most appropriate ones for this project, i.e. its RGB camera, IR/Depth Camera, Disparity, etc.
  * Determine "mask" of quadrotor that would be in the Kinect's FOV and adjust the acquired image accordingly, i.e. find how it changes when the Propeller's are spinning.
  * Implement obstacle tracking algorithm and show tracked obstacles in a GUI window. (DONE)
  * Transmit obstacle locations/trajectories to the microcontroller. (DONE)

== OA_Sonic ==

= Introduction =

== Summary ==
This project builds upon the previous project, [ICARUS_SLAM].  This project also uses the Quickstart and QuickstartPlus, along with Ultrasonic sensors, Microsoft Kinect and an AI algorithm to provide obstacle avoidance.  This can be used in cluttered environments, such as office spaces.

This project is intended for the UIC class CS 411: Artificial Intelligence and ECE 415: Image Analysis and Computer Vision 1

== Basic Operation ==

This project employs obstacle avoidance using the Microsoft Kinect and Ultrasonic Sensors placed around the UAV, due to advantages and disadvantages of each sensor.

The Ultrasonic Sensors have very little resolution, as they only report back the distance to the nearest obstacle they see.  But, since they have a fairly wide field of view and they consume little system resources (memory, space, weight, energy, etc) and they respond quickly to changes in their environment they are used to provide immediate feedback to the AI algorithm used to adjust the motor speeds to avoid obstacles they perceive.

The Kinect Sensor has a Visible and an IR Laser Array/Camera onboard.  For the purposes of this project only the IR Laser Array/Camera is used.  The Kinect provides a much more detailed view of what the UAV sees in front of itself.  Since it is assumed that the UAV will primarily be flying forward, the optimal placement of the Kinect of the view where the UAV will be flying is towards the front of the UAV.  The Kinect is able to much more accurately tell where obstacles are in its FOV, to provide guidance to the Mission Controller not only that there are obstacles but what direction would be the best to avoid them. 



[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/SystemDiagram_Simple.png]
Simplified Diagram

== OA_Sonic == 
=== AI Algorithm ===
A breadth first (tree) search algorithm will be used, until an appropriate heuristic can be determined (to implement an A* search algorithm).
==== Initial State ====
  PWM Inputs: {Front Motor=1500, Left Motor=1500, Back Motor=1500, Right Motor=1500}
==== Actions ====
  * PWM Outputs: [Front Motor, Left Motor, Back Motor, Right Motor]

==== Transition Model ====
switch Ultrasonic Distance
  Left:
    PWM_Out=Roll_Right(PWM_Input,Distance)
  Right:
    PWM_Out=Roll_Left(PWM_Input,Distance)
  Front:
    PWM_Out=Pitch_Back(PWM_Input,Distance)
  Back:
    PWM_Out=Pitch_Forward(PWM_Input,Distance)
  Top:
    PWM_Out=Reduce_Throttle(PWM_Input,Distance)
  Bottom:
    PWM_Out=Increase_Throttle(PWM_Input,Distance)

In reality, each function pair could be reduced to one function,i.e. Roll_Right() and Roll_Left() could be reduced to Roll().
==== Goal Test ====
  * Ultrasonic Distances: [Front>50, Left>50, Back>50, Right>50, Top>50, Bottom>50]

==== States ====
  * PWM Inputs: [Front Motor, Left Motor, Back Motor, Right Motor]
  * Ultrasonic Distances: [Front, Left, Back, Right, Top, Bottom]


== OA_Kinect == 
=== Visible or Depth? ===
As the Kinect has several different imaging capabilities, it is necessary to select the one that will have the best results for the AI algorithm.  Below is a sequence of pictures, in a cluttered environment, of an obstacle placed in the foreground of the Kinect.  The picture on the left is an image taken directly from the RGB camera, while the picture on the right is one taken from the IR/Depth Camera.

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/InitialImages/image1.jpg]
No obstacles here, just the environment.

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/InitialImages/image2.jpg]
A small wooded rod.

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/InitialImages/image3.jpg]
A larger wooden board.

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/InitialImages/image4.jpg]
The same wooden board again.

Obviously, the IR/Depth Camera is much more able to detect the difference between an obstacle in the foreground and an obstacle in the background in the Kinect's FOV.  Also, the IR camera will work just as well in a dark environment.

=== Analysis Algorithm Results ===
For each of the below images, the picture on the left is the visible light image, the middle image is the depth image, and the right image is the image generated by the OA_Kinect Algorithm.

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/AlgorithmImages/image0.png]
The depth image is made up of a 640x480 grid of pixels, with each pixel having the value in meters to 
The depth image is split into a 3x3 grid, with the closest depth measurement in each sector defining the gray level in the OA_Kinect grid.  The distance measurement for each of these sectors is displayed on this grid as well.

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/AlgorithmImages/image1.png]
The closer an object is to the camera, the darker it appears in this view of the OA_Kinect image.   

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/AlgorithmImages/image2.png]
When every point of the OA_Kinect image is less than the minimum distance that the Kinect will measure, the sector turns Red.

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/AlgorithmImages/image3.png]
Of course, we are not limited to a 3x3 grid.  Here a 25x25 grid is generated, with a significant performance hit.

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/AlgorithmImages/screenshot3.png]
When each OA_Kinect image is generated, the distance measurements of each sector (9 total here) are transmitted using the ICARUS communications protocol ($CAM,DIST Message) to the Motion Controller, which will then use this information along with the PWM inputs from the Flight Controller to modify the motor outputs.

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/AlgorithmImages/screenshot5.png]
Here is a 15x15 grid.  Although the frame rate for this is not too bad, the $CAM,DIST message now contains 225 values that need to be transmitted, which is quite large.

== OA_Sonic ==

= Media =

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/Flyer_Assy.png]
ICARUS Flyer

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/Flyer_Assy_Detail.png]
ICARUS Flyer Detail

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/Flyer_AssywUltrasonicBeams.png]

ICARUS Flyer w/ Ultrasonic Beams

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/Flyer_AssywKinectFOV.png]

ICARUS Flyer w/ Kinect Field of View for Visible and IR Camera

[http://dgitz.ipower.com/ICARUSRepo/Media/Projects/ICARUS_OA/Flyer_AssywEverything.png]

ICARUS Flyer w/ Kinect Field of View and Ultrasonic Beams

[https://www.dropbox.com/s/3723pg8ld2cp65k/ProgramFlowchart.pptx Software Flowchart]

[https://www.dropbox.com/s/qtkywga5dyuwr1r/Software%20Documentation.docx Software Documentation]

= Installation/Development Instructions =

== Compiling ==
{{{

}}}


== Commit ==
 # Add any uncommitted files:
{{{
git add .
}}}
 # Make a comment on the commit and then push it to the main branch.
{{{
git commit -m "Comment Text"
git push -u origin master
}}}

= Operation =
== Headless ==
 * In Terminal 1:
{{{
roscore
}}}

 * In Terminal 2:
{{{
roslaunch oa slow_computer2.launch
}}}
 
 * In Terminal 3:
{{{
sudo chmod 0777 /dev/ttyACM0
}}}

Execute the main program with the following:
{{{
roscd icarus_oa
python oa/nodes/primarycontroller.py
}}}
The following options are available.  Use the text in the <> where applicable, | signifies different options, and omit the <> and |:
 * Connection to GCS (Transmit GPS and Attitude Completed)
{{{
--gcs-device-type=<udp|Serial>
--gcs-device=<Device> #Where Device is an IP address if the type is udp, or a Serial Device such as /dev/ttyUSB0 or /dev/ttyACM0
--gcs-device-speed=<Speed> #Where Speed is a compatible baud rate if the type is Serial (default is 115200) and if not used the type is udp 
}}}
 * Connection to Flight Controller (Receive Attitude Completed)
{{{
--fc-device-type=<udp|Serial>
--fc-device=<Device> #Where Device is an IP address if the type is udp, or a Serial Device such as /dev/ttyUSB0 or /dev/ttyACM0
--fc-device-speed=<Speed> #Where Speed is a compatible baud rate if the type is Serial (default is 115200) and not used the type is udp 
}}}
 * Connection to Flight Controller GPS
{{{
--fcgps-device-type=<udp|Serial>
--fcgps-device=<Device> #Where Device is an IP address if the type is udp, or a Serial Device such as /dev/ttyUSB0 or /dev/ttyACM0
--fcgps-device-speed=<Speed> #Where Speed is a compatible baud rate if the type is Serial (default is 115200) and not used the type is udp 
}}}
 * Connection to Motion Controller
{{{
--mc-device-type=<udp|Serial>
--mc-device=<Device> #Where Device is an IP address if the type is udp, or a Serial Device such as /dev/ttyUSB0 or /dev/ttyACM0
--mc-device-speed=<Speed> #Where Speed is a compatible baud rate if the type is Serial (default is 115200) and not used the type is udp 
}}}
 * Connection to Remote (Transmit GPS and Attitude In Progress)
{{{
--remote-device-type=<udp|Serial>
--remote-device=<Device> #Where Device is an IP address if the type is udp, or a Serial Device such as /dev/ttyUSB0 or /dev/ttyACM0
--remote-device-speed=<Speed> #Where Speed is a compatible baud rate if the type is Serial (default is 115200) and not used the type is udp 
}}}

 * Example
{{{
oa/nodes/primarycontroller.py --fcgps-device-type=Serial --fcgps-device=/dev/ttyUSB0 --fc-device-type=Serial --fc-device=/dev/ttyACM0 --gcs-device-type=udp --gcs-device=10.7.45.208
}}}

= References =
[http://wiki.python.org/moin/PythonForArtificialIntelligence]